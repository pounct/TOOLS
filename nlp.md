
# Procesamiento del lenguaje natural (PLN), Transformers y LLM
## PLN (Procesamiento del lenguaje natural) tiene como objetivo dotar a las computadoras de la capacidad de analizar, comprender y generar lenguaje humano.

## Antes de 2017, el PLN utilizaba técnicas de aprendizaje profundo basadas en modelos de redes neuronales como RNN, LSTM o GRU.

## En 2017, el artículo "Attention is All You Need" introdujo los Transformers, que se basan en el mecanismo de atención para lograr un avance significativo que mejora considerablemente el rendimiento de los modelos de PLN.

# Modelos destacados:

## BERT (Bidirectional Encoder Representations from Transformers), Google
## GPT (Generative Pre-trained Transformers), OpenAI
## Los modelos de lenguaje grande (LLM), basados en transformers, se entrenan utilizando un gran corpus de datos y recursos de computación de alto rendimiento (HPC) de la computación en la nube.

## Los LLM exhiben un rendimiento de última generación en tareas de PLN que se dividen en dos categorías generales:

### Generación de texto a etiqueta (clasificación):

- Análisis de sentimiento
### Generación de texto a texto:

- Traducción de texto
- Resumen de texto

## ¿Cómo funcionan?

### Los Transformers utilizan el mecanismo de atención para "prestar atención" a diferentes partes de una entrada de texto, lo que les permite comprender mejor el contexto y la relación entre las palabras. Esta capacidad les da una gran ventaja sobre los modelos anteriores como RNN, que procesaban las palabras de forma secuencial.

### Los LLM se entrenan en grandes conjuntos de datos de texto, lo que les permite aprender una amplia gama de patrones y relaciones lingüísticas. Esto les da la capacidad de realizar una variedad de tareas de PLN con un alto grado de precisión.
